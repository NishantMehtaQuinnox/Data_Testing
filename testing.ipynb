{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(1044, \"Access denied for user 'myuser'@'localhost' to database 'data_testing'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m conn_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Replace with your host address\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyuser\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Replace with your username\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmypassword\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Replace with your password\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_testing\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your database name\u001b[39;00m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Establish a database connection\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconn_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:358\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:664\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_seq_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_server_information()\n\u001b[0;32m--> 664\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_authentication\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# Send \"SET NAMES\" query on init for:\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# - Ensure charaset (and collation) is set to the server.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#   - collation_id in handshake packet may be ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;66;03m# - https://github.com/wagtail/wagtail/issues/9477\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# - https://zenn.dev/methane/articles/2023-mysql-collation (Japanese)\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_character_set(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollation)\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:954\u001b[0m, in \u001b[0;36mConnection._request_authentication\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _lenenc_int(\u001b[38;5;28mlen\u001b[39m(connect_attrs)) \u001b[38;5;241m+\u001b[39m connect_attrs\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_packet(data)\n\u001b[0;32m--> 954\u001b[0m auth_packet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# if authentication method isn't accepted the first byte\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# will have the octet 254\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth_packet\u001b[38;5;241m.\u001b[39mis_auth_switch_request():\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:772\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39munbuffered_active \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39munbuffered_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 772\u001b[0m     \u001b[43mpacket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packet\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/protocol.py:221\u001b[0m, in \u001b[0;36mMysqlPacket.raise_for_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno =\u001b[39m\u001b[38;5;124m\"\u001b[39m, errno)\n\u001b[0;32m--> 221\u001b[0m \u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_mysql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/err.py:143\u001b[0m, in \u001b[0;36mraise_mysql_exception\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errorclass \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     errorclass \u001b[38;5;241m=\u001b[39m InternalError \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m OperationalError\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m errorclass(errno, errval)\n",
      "\u001b[0;31mOperationalError\u001b[0m: (1044, \"Access denied for user 'myuser'@'localhost' to database 'data_testing'\")"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "# Database connection parameters\n",
    "conn_params = {\n",
    "    'host': '127.0.0.1',  # Replace with your host address\n",
    "    'user': 'myuser',  # Replace with your username\n",
    "    'password': 'mypassword',  # Replace with your password\n",
    "    'db': 'data_testing'  # Replace with your database name\n",
    "}\n",
    "# Establish a database connection\n",
    "conn = pymysql.connect(**conn_params)\n",
    "cur = conn.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "(0, '')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM transactions LIMIT 100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/cursors.py:153\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    151\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmogrify(query, args)\n\u001b[0;32m--> 153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/cursors.py:322\u001b[0m, in \u001b[0;36mCursor._query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    320\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_db()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_result()\n\u001b[0;32m--> 322\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_get_result()\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrowcount\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:557\u001b[0m, in \u001b[0;36mConnection.query\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    556\u001b[0m     sql \u001b[38;5;241m=\u001b[39m sql\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMMAND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOM_QUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_query_result(unbuffered\u001b[38;5;241m=\u001b[39munbuffered)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows\n",
      "File \u001b[0;32m~/anaconda3/envs/data_testing/lib/python3.10/site-packages/pymysql/connections.py:840\u001b[0m, in \u001b[0;36mConnection._execute_command\u001b[0;34m(self, command, sql)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;124;03m:raise InterfaceError: If the connection is closed.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;124;03m:raise ValueError: If no username was specified.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock:\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\u001b[38;5;241m.\u001b[39mInterfaceError(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# If the last query was unbuffered, make sure it finishes before\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# sending new commands\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInterfaceError\u001b[0m: (0, '')"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT * FROM transactions LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of records to modify\n",
    "number_of_records_to_modify = 100000\n",
    "\n",
    "# Define a function to generate a random value\n",
    "# This can be customized to generate the type of random data you need\n",
    "def modify_string(s):\n",
    "    if not s.strip():  # checking for a string that isn't just whitespace\n",
    "        return s\n",
    "    random_index = random.randrange(len(s))\n",
    "    new_char = random.choice(string.ascii_letters)\n",
    "    while s[random_index] == new_char or s[random_index].isspace():  # prevent replacing with the same character or space\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    return s\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9000):\n",
    "    if i == 999:\n",
    "        print(i)\n",
    "    # Generate a random row index\n",
    "    random_row_index = np.random.randint(len(df))\n",
    "    # Generate a random column index\n",
    "\n",
    "    random_col_index = np.random.choice([2,3,4])\n",
    "    # Get the column name\n",
    "    random_col_name = df.columns[random_col_index]\n",
    "    # Get the cell value\n",
    "    cell_value = df.at[random_row_index, random_col_name]\n",
    "    # Check if the value is a string and non-empty/non-whitespace\n",
    "    if isinstance(cell_value, str) and cell_value.strip():\n",
    "        # Modify the cell value\n",
    "        print(\"Mod\",cell_value)\n",
    "        df.at[random_row_index, random_col_name] = modify_string(cell_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'acc_code': 'account_code'}, inplace=True)\n",
    "df.rename(columns={'cust_code': 'customer_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "from extract.data_loader import JsonToGDFLoader\n",
    "from evaluate.validator import DataValidator\n",
    "\n",
    "# Define your transformations list as follows:\n",
    "transformations = [\n",
    "    {\n",
    "        'from_column_name': 'original_column1',\n",
    "        'to_column_name': 'new_column1',\n",
    "        'type_update': 'int64',\n",
    "        'transformation_function': lambda x: x * 2\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'original_column2',\n",
    "        'to_column_name': 'new_column2',\n",
    "        'type_update': None,\n",
    "        'transformation_function': lambda x: x.strip() if isinstance(x, str) else x\n",
    "    }\n",
    "    # Add as many dictionaries as needed for transformations\n",
    "]\n",
    "\n",
    "folder_url = 'data_testing/7bc4e498-e698-11ee-9eb5-025f58bc16f6'\n",
    "loader = JsonToGDFLoader(folder_url)\n",
    "df = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DataValidator(df)\n",
    "\n",
    "# Define your validations\n",
    "filename_regex = r'^data_\\\\d{4}.json$'\n",
    "file_size_limit_mb = 10\n",
    "file_format = '.json'\n",
    "expected_record_count = 7740001\n",
    "expected_data_types = {\n",
    "    'tx_id': 'int64', \n",
    "    'customer_code': 'object',\n",
    "    # Other columns and their expected types...\n",
    "}\n",
    "expected_not_null_columns = ['tx_type', 'tx_category']\n",
    "expected_field_length = {\n",
    "    'tx_type': (0, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = validator.generate_report(expected_count=expected_record_count,check_null_columns=expected_not_null_columns)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_string(s):\n",
    "    print(s)\n",
    "    if not s:\n",
    "        return s\n",
    "    while True:\n",
    "        random_index = random.randrange(len(s))\n",
    "        if s[random_index] not in [\"\",\" \"]:\n",
    "            break\n",
    "    while True:\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "        if s[random_index] != new_char:\n",
    "            break\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "num_changes = 10000\n",
    "\n",
    "updated_df = txt_base_csv[txt_base_csv['Length'] > 45]\n",
    "rows_to_modify = updated_df.sample(num_changes).index\n",
    "\n",
    "txt_base_csv.loc[rows_to_modify, 'Data'] = txt_base_csv.loc[rows_to_modify, 'Data'].apply(modify_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.drop('Length', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv.to_csv(\"Input/Input_Stg_Big_With_Diff.txt\",sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv['Length'] = txt_base_csv['Data'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_update = txt_base_csv.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_to_update['Data'] = sample_to_update['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diff = 10  # Number of differences the user wants.\n",
    "mask = txt_base_csv['Length'] >= 45\n",
    "txt_base_csv.loc[mask,'New'] = txt_base_csv['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_base_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_char_modifications(df, num_changes):\n",
    "    # Make sure there are enough rows to modify\n",
    "    assert num_changes <= len(df), \"Number of changes exceeds number of rows.\"\n",
    "\n",
    "    # Generate random row indices\n",
    "    random_indices = cp.random.choice(len(df), num_changes, replace=False)\n",
    "\n",
    "    # Function to randomly modify a character in a string\n",
    "    def modify_random_char(s):\n",
    "        if len(s) > 0:\n",
    "            pos_to_modify = random.randint(0, len(s) - 1)\n",
    "            random_char = random.choice(string.ascii_letters)\n",
    "            return s[:pos_to_modify] + random_char + s[pos_to_modify + 1:]\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    df.loc[cudf.Series(random_indices), \"Data\"] = df.loc[cudf.Series(random_indices), \"Data\"].str\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_url = 'data_testing/f5466802-e682-11ee-a815-025f58bc16f6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "# Instantiate S3Utils\n",
    "s3utils = S3Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = s3utils.list_files(folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_csvs_to_df_with_cudf(urls):\n",
    "    # Ensure URLs list is not empty\n",
    "    if not urls:\n",
    "        return cudf.DataFrame()\n",
    "    \n",
    "    # Process the first URL to get the headers\n",
    "    gdf_list = []\n",
    "    headers = None\n",
    "    content = s3utils.download_file_get_content(urls[0])\n",
    "    if content:\n",
    "        # Read CSV file into cuDF DataFrame and get headers\n",
    "        gdf = cudf.read_csv(io.BytesIO(content))\n",
    "        headers = gdf.columns.tolist()\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Dictionary to store Future to URL mapping (excluding the first URL)\n",
    "    future_to_url = {}\n",
    "    \n",
    "    # Use ThreadPoolExecutor to fetch and load CSV files in parallel\n",
    "    max_workers = os.cpu_count() - 1 or 1\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        for url in urls[1:]:  # Start from the second URL, since first is already processed\n",
    "            future = executor.submit(s3utils.download_file_get_content, url)\n",
    "            future_to_url[future] = url\n",
    "            \n",
    "        # Process as they complete\n",
    "        for future in as_completed(future_to_url):\n",
    "            content = future.result()\n",
    "            if content:\n",
    "                # Use the same headers obtained from the first file\n",
    "                gdf = cudf.read_csv(\n",
    "                    io.BytesIO(content),\n",
    "                    names=headers      # Skip the header row found in the file\n",
    "                )\n",
    "                gdf_list.append(gdf)\n",
    "                \n",
    "    # Concatenate all DataFrames into a single one\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV files using extracted headers from part_0 into a single cuDF DataFrame\n",
    "gdf = load_csvs_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_and_load_json(url):\n",
    "    content = s3utils.download_file_get_content(url)\n",
    "    if content:\n",
    "        return cudf.read_json(io.BytesIO(content), lines=True)\n",
    "    return None\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls, max_workers=4):\n",
    "    # Use ThreadPoolExecutor to download files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        gdf_futures = {executor.submit(download_and_load_json, url): url for url in urls}\n",
    "    \n",
    "    gdf_list = []\n",
    "    for future in as_completed(gdf_futures):\n",
    "        gdf = future.result()\n",
    "        if gdf is not None:\n",
    "            gdf_list.append(gdf)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single one only if gdf_list is not empty\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "list_urls = [\n",
    "    'https://s3.amazonaws.com/mybucket/data1.json',\n",
    "    'https://s3.amazonaws.com/mybucket/data2.json',\n",
    "    # ... more URLs ...\n",
    "]\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls, max_workers=10)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "s3utils = S3Utils()\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls):\n",
    "    # List to store each DataFrame\n",
    "    gdf_list = []\n",
    "    \n",
    "    # Iterate over all S3 URLs\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "        # Assuming `download_file_get_content` downloads the file content as a bytes object\n",
    "        content = s3utils.download_file_get_content(url)\n",
    "        if content:\n",
    "            # Read JSON file into cuDF DataFrame\n",
    "            # since `cudf.read_json` expects a JSON lines format, we directly use `content`\n",
    "            gdf_list.append(cudf.read_json(io.BytesIO(content), lines=True))\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    # If there's anything in the list, use cudf.concat, otherwise just create an empty DataFrame\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "# `list_urls` should be a list of presigned S3 URLs pointing to the JSON files\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"2.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "gdf = cudf.read_csv(\"mock_data.csv\")\n",
    "print(gdf.head()) # Print the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_gdf(gdf, transformations):\n",
    "    for transform in transformations:\n",
    "        from_column = transform.get('from_column_name')\n",
    "        to_column = transform.get('to_column_name')\n",
    "        type_update = transform.get('type_update')  # Get the type if it exists\n",
    "        transformation_function = transform.get('transformation_function')\n",
    "\n",
    "        if transformation_function and from_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[from_column].applymap(transformation_function)\n",
    "\n",
    "        # Rename the column if necessary\n",
    "        if from_column in gdf.columns and from_column != to_column:\n",
    "            gdf = gdf.rename(columns={from_column: to_column})\n",
    "            \n",
    "        # Update column type if type_update is provided\n",
    "        if type_update and to_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[to_column].astype(type_update)\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    {\n",
    "        'from_column_name': 'create_time',\n",
    "        'to_column_name': 'created_time'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'email',\n",
    "        'to_column_name': 'email_id'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'phone',\n",
    "        'to_column_name': 'phone',\n",
    "        'type_update':'float64'\n",
    "    }\n",
    "    \n",
    "    # Add as many dictionaries as needed for transformations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcdf = transform_gdf(gdf,transformations)\n",
    "print(tcdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txt_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
