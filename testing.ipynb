{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of records to modify\n",
    "number_of_records_to_modify = 100000\n",
    "\n",
    "# Define a function to generate a random value\n",
    "# This can be customized to generate the type of random data you need\n",
    "def modify_string(s):\n",
    "    if not s.strip():  # checking for a string that isn't just whitespace\n",
    "        return s\n",
    "    random_index = random.randrange(len(s))\n",
    "    new_char = random.choice(string.ascii_letters)\n",
    "    while s[random_index] == new_char or s[random_index].isspace():  # prevent replacing with the same character or space\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    return s\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9000):\n",
    "    if i == 999:\n",
    "        print(i)\n",
    "    # Generate a random row index\n",
    "    random_row_index = np.random.randint(len(df))\n",
    "    # Generate a random column index\n",
    "    random_col_index = np.random.choice([2,3,4])\n",
    "    # Get the column name\n",
    "    random_col_name = df.columns[random_col_index]\n",
    "    # Get the cell value\n",
    "    cell_value = df.at[random_row_index, random_col_name]\n",
    "    # Check if the value is a string and non-empty/non-whitespace\n",
    "    if isinstance(cell_value, str) and cell_value.strip():\n",
    "        # Modify the cell value\n",
    "        print(\"Mod\",cell_value)\n",
    "        df.at[random_row_index, random_col_name] = modify_string(cell_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'acc_code': 'account_code'}, inplace=True)\n",
    "df.rename(columns={'cust_code': 'customer_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of records to modify\n",
    "number_of_records_to_modify = 5\n",
    "\n",
    "# Define a function to generate a random value\n",
    "# This can be customized to generate the type of random data you need\n",
    "def modify_string(s):\n",
    "    if not s.strip():  # checking for a string that isn't just whitespace\n",
    "        return s\n",
    "    random_index = random.randrange(len(s))\n",
    "    new_char = random.choice(string.ascii_letters)\n",
    "    while s[random_index] == new_char or s[random_index].isspace():  # prevent replacing with the same character or space\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    return s\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Choose n random indices from the DataFrame to be modified\n",
    "# and make sure they are unique if the DataFrame is not too small.\n",
    "if len(df) < number_of_records_to_modify:\n",
    "    print(\"There are not enough records in the CSV to modify.\")\n",
    "    n_records = len(df)\n",
    "else:\n",
    "    n_records = number_of_records_to_modify\n",
    "\n",
    "indices_to_modify = np.random.choice(df.index, size=n_records, replace=False)\n",
    "\n",
    "# Loop through each record to modify and assign the random value\n",
    "for col in df.columns:\n",
    "    df.loc[indices_to_modify, col] = [generate_random_value() for _ in range(n_records)]\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(f'Modified {n_records} records with random values and saved to output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv['Length'] = txt_base_csv['Data'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_string(s):\n",
    "    print(s)\n",
    "    if not s:\n",
    "        return s\n",
    "    while True:\n",
    "        random_index = random.randrange(len(s))\n",
    "        if s[random_index] not in [\"\",\" \"]:\n",
    "            break\n",
    "    while True:\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "        if s[random_index] != new_char:\n",
    "            break\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "num_changes = 10000\n",
    "\n",
    "updated_df = txt_base_csv[txt_base_csv['Length'] > 45]\n",
    "rows_to_modify = updated_df.sample(num_changes).index\n",
    "\n",
    "txt_base_csv.loc[rows_to_modify, 'Data'] = txt_base_csv.loc[rows_to_modify, 'Data'].apply(modify_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.drop('Length', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv.to_csv(\"Input/Input_Stg_Big_With_Diff.txt\",sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv['Length'] = txt_base_csv['Data'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_update = txt_base_csv.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_to_update['Data'] = sample_to_update['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diff = 10  # Number of differences the user wants.\n",
    "mask = txt_base_csv['Length'] >= 45\n",
    "txt_base_csv.loc[mask,'New'] = txt_base_csv['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_base_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_char_modifications(df, num_changes):\n",
    "    # Make sure there are enough rows to modify\n",
    "    assert num_changes <= len(df), \"Number of changes exceeds number of rows.\"\n",
    "\n",
    "    # Generate random row indices\n",
    "    random_indices = cp.random.choice(len(df), num_changes, replace=False)\n",
    "\n",
    "    # Function to randomly modify a character in a string\n",
    "    def modify_random_char(s):\n",
    "        if len(s) > 0:\n",
    "            pos_to_modify = random.randint(0, len(s) - 1)\n",
    "            random_char = random.choice(string.ascii_letters)\n",
    "            return s[:pos_to_modify] + random_char + s[pos_to_modify + 1:]\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    df.loc[cudf.Series(random_indices), \"Data\"] = df.loc[cudf.Series(random_indices), \"Data\"].str\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_url = 'data_testing/f5466802-e682-11ee-a815-025f58bc16f6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "# Instantiate S3Utils\n",
    "s3utils = S3Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = s3utils.list_files(folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai-services-stg.s3.amazonaws.com/data_testing/f5466802-e682-11ee-a815-025f58bc16f6/f5466802-e682-11ee-a815-025f58bc16f6_part_0.json?AWSAccessKeyId=AKIAUI4MYPS6PXLI2Y6F&Signature=KefboeuTK7rw8Ps13xQtlOgTr78%3D&Expires=1711001814\n"
     ]
    }
   ],
   "source": [
    "print(list_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_csvs_to_df_with_cudf(urls):\n",
    "    # Ensure URLs list is not empty\n",
    "    if not urls:\n",
    "        return cudf.DataFrame()\n",
    "    \n",
    "    # Process the first URL to get the headers\n",
    "    gdf_list = []\n",
    "    headers = None\n",
    "    content = s3utils.download_file_get_content(urls[0])\n",
    "    if content:\n",
    "        # Read CSV file into cuDF DataFrame and get headers\n",
    "        gdf = cudf.read_csv(io.BytesIO(content))\n",
    "        headers = gdf.columns.tolist()\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Dictionary to store Future to URL mapping (excluding the first URL)\n",
    "    future_to_url = {}\n",
    "    \n",
    "    # Use ThreadPoolExecutor to fetch and load CSV files in parallel\n",
    "    max_workers = os.cpu_count() - 1 or 1\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        for url in urls[1:]:  # Start from the second URL, since first is already processed\n",
    "            future = executor.submit(s3utils.download_file_get_content, url)\n",
    "            future_to_url[future] = url\n",
    "            \n",
    "        # Process as they complete\n",
    "        for future in as_completed(future_to_url):\n",
    "            content = future.result()\n",
    "            if content:\n",
    "                # Use the same headers obtained from the first file\n",
    "                gdf = cudf.read_csv(\n",
    "                    io.BytesIO(content),\n",
    "                    names=headers      # Skip the header row found in the file\n",
    "                )\n",
    "                gdf_list.append(gdf)\n",
    "                \n",
    "    # Concatenate all DataFrames into a single one\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV files using extracted headers from part_0 into a single cuDF DataFrame\n",
    "gdf = load_csvs_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_and_load_json(url):\n",
    "    content = s3utils.download_file_get_content(url)\n",
    "    if content:\n",
    "        return cudf.read_json(io.BytesIO(content), lines=True)\n",
    "    return None\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls, max_workers=4):\n",
    "    # Use ThreadPoolExecutor to download files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        gdf_futures = {executor.submit(download_and_load_json, url): url for url in urls}\n",
    "    \n",
    "    gdf_list = []\n",
    "    for future in as_completed(gdf_futures):\n",
    "        gdf = future.result()\n",
    "        if gdf is not None:\n",
    "            gdf_list.append(gdf)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single one only if gdf_list is not empty\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "list_urls = [\n",
    "    'https://s3.amazonaws.com/mybucket/data1.json',\n",
    "    'https://s3.amazonaws.com/mybucket/data2.json',\n",
    "    # ... more URLs ...\n",
    "]\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls, max_workers=10)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "s3utils = S3Utils()\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls):\n",
    "    # List to store each DataFrame\n",
    "    gdf_list = []\n",
    "    \n",
    "    # Iterate over all S3 URLs\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "        # Assuming `download_file_get_content` downloads the file content as a bytes object\n",
    "        content = s3utils.download_file_get_content(url)\n",
    "        if content:\n",
    "            # Read JSON file into cuDF DataFrame\n",
    "            # since `cudf.read_json` expects a JSON lines format, we directly use `content`\n",
    "            gdf_list.append(cudf.read_json(io.BytesIO(content), lines=True))\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    # If there's anything in the list, use cudf.concat, otherwise just create an empty DataFrame\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "# `list_urls` should be a list of presigned S3 URLs pointing to the JSON files\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"2.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          create_time           name                      email  \\\n",
      "0  11  2023-04-01 12:00:00       John Doe       john.doe@example.com   \n",
      "1  12  2023-04-01 12:10:00     Jane Smith     jane.smith@example.com   \n",
      "2  13  2023-04-01 12:20:00  Alice Johnson  alice.johnson@example.com   \n",
      "3  14  2023-04-01 12:30:00     Mike Brown     mike.brown@example.com   \n",
      "4  15  2023-04-01 12:40:00    Emma Wilson    emma.wilson@example.com   \n",
      "\n",
      "        phone  \n",
      "0  1234567890  \n",
      "1  2345678901  \n",
      "2  3456789012  \n",
      "3  4567890123  \n",
      "4  5678901234  \n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "gdf = cudf.read_csv(\"mock_data.csv\")\n",
    "print(gdf.head()) # Print the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_gdf(gdf, transformations):\n",
    "    for transform in transformations:\n",
    "        from_column = transform.get('from_column_name')\n",
    "        to_column = transform.get('to_column_name')\n",
    "        type_update = transform.get('type_update')  # Get the type if it exists\n",
    "        transformation_function = transform.get('transformation_function')\n",
    "\n",
    "        if transformation_function and from_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[from_column].applymap(transformation_function)\n",
    "\n",
    "        # Rename the column if necessary\n",
    "        if from_column in gdf.columns and from_column != to_column:\n",
    "            gdf = gdf.rename(columns={from_column: to_column})\n",
    "            \n",
    "        # Update column type if type_update is provided\n",
    "        if type_update and to_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[to_column].astype(type_update)\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    {\n",
    "        'from_column_name': 'create_time',\n",
    "        'to_column_name': 'created_time'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'email',\n",
    "        'to_column_name': 'email_id'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'phone',\n",
    "        'to_column_name': 'phone',\n",
    "        'type_update':'float64'\n",
    "    }\n",
    "    \n",
    "    # Add as many dictionaries as needed for transformations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         created_time           name                   email_id  \\\n",
      "0  11  2023-04-01 12:00:00       John Doe       john.doe@example.com   \n",
      "1  12  2023-04-01 12:10:00     Jane Smith     jane.smith@example.com   \n",
      "2  13  2023-04-01 12:20:00  Alice Johnson  alice.johnson@example.com   \n",
      "3  14  2023-04-01 12:30:00     Mike Brown     mike.brown@example.com   \n",
      "4  15  2023-04-01 12:40:00    Emma Wilson    emma.wilson@example.com   \n",
      "\n",
      "          phone  \n",
      "0  1.234568e+09  \n",
      "1  2.345679e+09  \n",
      "2  3.456789e+09  \n",
      "3  4.567890e+09  \n",
      "4  5.678901e+09  \n"
     ]
    }
   ],
   "source": [
    "tcdf = transform_gdf(gdf,transformations)\n",
    "print(tcdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txt_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
