{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of records to modify\n",
    "number_of_records_to_modify = 100000\n",
    "\n",
    "# Define a function to generate a random value\n",
    "# This can be customized to generate the type of random data you need\n",
    "def modify_string(s):\n",
    "    if not s.strip():  # checking for a string that isn't just whitespace\n",
    "        return s\n",
    "    random_index = random.randrange(len(s))\n",
    "    new_char = random.choice(string.ascii_letters)\n",
    "    while s[random_index] == new_char or s[random_index].isspace():  # prevent replacing with the same character or space\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    return s\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9000):\n",
    "    if i == 999:\n",
    "        print(i)\n",
    "    # Generate a random row index\n",
    "    random_row_index = np.random.randint(len(df))\n",
    "    # Generate a random column index\n",
    "    random_col_index = np.random.choice([2,3,4])\n",
    "    # Get the column name\n",
    "    random_col_name = df.columns[random_col_index]\n",
    "    # Get the cell value\n",
    "    cell_value = df.at[random_row_index, random_col_name]\n",
    "    # Check if the value is a string and non-empty/non-whitespace\n",
    "    if isinstance(cell_value, str) and cell_value.strip():\n",
    "        # Modify the cell value\n",
    "        print(\"Mod\",cell_value)\n",
    "        df.at[random_row_index, random_col_name] = modify_string(cell_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'acc_code': 'account_code'}, inplace=True)\n",
    "df.rename(columns={'cust_code': 'customer_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "from data_loader import JsonToGDFLoader\n",
    "from validator import DataValidator\n",
    "\n",
    "# Define your transformations list as follows:\n",
    "transformations = [\n",
    "    {\n",
    "        'from_column_name': 'original_column1',\n",
    "        'to_column_name': 'new_column1',\n",
    "        'type_update': 'int64',\n",
    "        'transformation_function': lambda x: x * 2\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'original_column2',\n",
    "        'to_column_name': 'new_column2',\n",
    "        'type_update': None,\n",
    "        'transformation_function': lambda x: x.strip() if isinstance(x, str) else x\n",
    "    }\n",
    "    # Add as many dictionaries as needed for transformations\n",
    "]\n",
    "\n",
    "folder_url = 'data_testing/7bc4e498-e698-11ee-9eb5-025f58bc16f6'\n",
    "loader = JsonToGDFLoader(folder_url)\n",
    "df = loader.load()\n",
    "\n",
    "df = loader.transform_gdf(df,transformations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DataValidator(df)\n",
    "\n",
    "# Define your validations\n",
    "filename_regex = r'^data_\\\\d{4}.json$'\n",
    "file_size_limit_mb = 10\n",
    "file_format = '.json'\n",
    "expected_record_count = 7740001\n",
    "expected_data_types = {\n",
    "    'tx_id': 'int64', \n",
    "    'customer_code': 'object',\n",
    "    # Other columns and their expected types...\n",
    "}\n",
    "expected_not_null_columns = ['tx_type', 'tx_category']\n",
    "expected_field_length = {\n",
    "    'tx_type': (0, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tx_id                                       account_code  \\\n",
      "0  1336894  ba935ba66e1e2e8ffbdc94643013d31aae98d47835acb2...   \n",
      "1  1336952  a03b6f79de10e9c284fc87b634c99b9ea623e140f64816...   \n",
      "2  1337397  0cc63f34b02c0b08cce2b6ca69828d117f7ea94201a623...   \n",
      "3  1337589  2da2c3de5891a53f4e7ba5e014b3f6f68f73d206190215...   \n",
      "4  1337649  a6076f7b5d1fdcaa9c5d6803870b5b1cf04c7684429030...   \n",
      "\n",
      "                                       customer_code tx_type tx_category  \\\n",
      "0  21562fbc87effbe241de14691576ce1dc9686d6505eb6a...      OU           R   \n",
      "1  29cf7e006799a0cd87914cced5e067aa9d86b00bcb18fe...      IN           P   \n",
      "2  57835ed6ec5a5d2703edbf0934f47a45ee7585145ab56a...      OU           R   \n",
      "3  f864d0a2f639b633ebfede99a4d2174c4bdb83ce64bc97...      OU           R   \n",
      "4  6a95e585c981bd95c8ad91be50db31c694329b8b3a0023...      IN           R   \n",
      "\n",
      "   tx_amount tx_currency    tx_datetime  tx_post_balance  \\\n",
      "0  4440.5605         USD  1586848017000       76357.1325   \n",
      "1  3483.3803         EUR  1700606382000       86681.2154   \n",
      "2  6309.9970         USD  1652156529000       26962.5501   \n",
      "3  9120.0991         EUR  1615437588000       71977.8637   \n",
      "4  2232.5079         GBP  1661106394000       88882.9979   \n",
      "\n",
      "                                        tx_reference tx_channel tx_status  \\\n",
      "0  f648bd68970ba1bf0595c7d0d3a378ca87a4cc87a23b70...          1         A   \n",
      "1  5f4ac5d74cb946c54cc3e3b219741c1e5d3e48092a9df3...          2         R   \n",
      "2  9381506296058b351e6b740cf4e93bea2e277b6ca56b0d...          2         A   \n",
      "3  ac7c3a9400c0bfbf0fd46818e8170e6d659349f431654d...          1         R   \n",
      "4  49d8afd44646e708d0bafda510d6f7e0229822362aaee0...          1         P   \n",
      "\n",
      "   fraud_flag  created_timestamp  updated_timestamp  \n",
      "0       False      1709900191489      1709900191489  \n",
      "1       False      1709900191489      1709900191489  \n",
      "2       False      1709900191489      1709900191489  \n",
      "3       False      1709900191489      1709900191489  \n",
      "4       False      1709900191489      1709900191489  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your cuDF DataFrame and 'data.json' is the file you're validating\n",
    "\n",
    "\n",
    "# Perform validations\n",
    "# assert validator.filename_matches_regex('data.json', filename_regex)\n",
    "# assert validator.file_size_within_limit('data.json', file_size_limit_mb)\n",
    "# assert validator.file_format_is_correct('data.json', file_format)\n",
    "assert validator.record_count_matches(expected_record_count)\n",
    "assert validator.column_data_types(expected_data_types)\n",
    "# assert not validator.duplicates_exist()\n",
    "assert validator.not_null_columns(expected_not_null_columns)\n",
    "assert validator.field_length_within_range('tx_type', *expected_field_length['tx_type'])\n",
    "# For custom comparators, provide the lambda or function\n",
    "# assert validator.custom_comparator('age', lambda ages: (ages > 0).all())\n",
    "\n",
    "# If any assertion fails, it will raise an AssertionError.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_string(s):\n",
    "    print(s)\n",
    "    if not s:\n",
    "        return s\n",
    "    while True:\n",
    "        random_index = random.randrange(len(s))\n",
    "        if s[random_index] not in [\"\",\" \"]:\n",
    "            break\n",
    "    while True:\n",
    "        new_char = random.choice(string.ascii_letters)\n",
    "        if s[random_index] != new_char:\n",
    "            break\n",
    "    s = s[:random_index] + new_char + s[random_index + 1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "num_changes = 10000\n",
    "\n",
    "updated_df = txt_base_csv[txt_base_csv['Length'] > 45]\n",
    "rows_to_modify = updated_df.sample(num_changes).index\n",
    "\n",
    "txt_base_csv.loc[rows_to_modify, 'Data'] = txt_base_csv.loc[rows_to_modify, 'Data'].apply(modify_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.drop('Length', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv.to_csv(\"Input/Input_Stg_Big_With_Diff.txt\",sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv['Length'] = txt_base_csv['Data'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_base_csv = txt_base_csv.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_update = txt_base_csv.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_to_update['Data'] = sample_to_update['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diff = 10  # Number of differences the user wants.\n",
    "mask = txt_base_csv['Length'] >= 45\n",
    "txt_base_csv.loc[mask,'New'] = txt_base_csv['Data'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_base_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_char_modifications(df, num_changes):\n",
    "    # Make sure there are enough rows to modify\n",
    "    assert num_changes <= len(df), \"Number of changes exceeds number of rows.\"\n",
    "\n",
    "    # Generate random row indices\n",
    "    random_indices = cp.random.choice(len(df), num_changes, replace=False)\n",
    "\n",
    "    # Function to randomly modify a character in a string\n",
    "    def modify_random_char(s):\n",
    "        if len(s) > 0:\n",
    "            pos_to_modify = random.randint(0, len(s) - 1)\n",
    "            random_char = random.choice(string.ascii_letters)\n",
    "            return s[:pos_to_modify] + random_char + s[pos_to_modify + 1:]\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    df.loc[cudf.Series(random_indices), \"Data\"] = df.loc[cudf.Series(random_indices), \"Data\"].str\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_url = 'data_testing/f5466802-e682-11ee-a815-025f58bc16f6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "# Instantiate S3Utils\n",
    "s3utils = S3Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = s3utils.list_files(folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_csvs_to_df_with_cudf(urls):\n",
    "    # Ensure URLs list is not empty\n",
    "    if not urls:\n",
    "        return cudf.DataFrame()\n",
    "    \n",
    "    # Process the first URL to get the headers\n",
    "    gdf_list = []\n",
    "    headers = None\n",
    "    content = s3utils.download_file_get_content(urls[0])\n",
    "    if content:\n",
    "        # Read CSV file into cuDF DataFrame and get headers\n",
    "        gdf = cudf.read_csv(io.BytesIO(content))\n",
    "        headers = gdf.columns.tolist()\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Dictionary to store Future to URL mapping (excluding the first URL)\n",
    "    future_to_url = {}\n",
    "    \n",
    "    # Use ThreadPoolExecutor to fetch and load CSV files in parallel\n",
    "    max_workers = os.cpu_count() - 1 or 1\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        for url in urls[1:]:  # Start from the second URL, since first is already processed\n",
    "            future = executor.submit(s3utils.download_file_get_content, url)\n",
    "            future_to_url[future] = url\n",
    "            \n",
    "        # Process as they complete\n",
    "        for future in as_completed(future_to_url):\n",
    "            content = future.result()\n",
    "            if content:\n",
    "                # Use the same headers obtained from the first file\n",
    "                gdf = cudf.read_csv(\n",
    "                    io.BytesIO(content),\n",
    "                    names=headers      # Skip the header row found in the file\n",
    "                )\n",
    "                gdf_list.append(gdf)\n",
    "                \n",
    "    # Concatenate all DataFrames into a single one\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV files using extracted headers from part_0 into a single cuDF DataFrame\n",
    "gdf = load_csvs_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_and_load_json(url):\n",
    "    content = s3utils.download_file_get_content(url)\n",
    "    if content:\n",
    "        return cudf.read_json(io.BytesIO(content), lines=True)\n",
    "    return None\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls, max_workers=4):\n",
    "    # Use ThreadPoolExecutor to download files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        gdf_futures = {executor.submit(download_and_load_json, url): url for url in urls}\n",
    "    \n",
    "    gdf_list = []\n",
    "    for future in as_completed(gdf_futures):\n",
    "        gdf = future.result()\n",
    "        if gdf is not None:\n",
    "            gdf_list.append(gdf)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single one only if gdf_list is not empty\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "list_urls = [\n",
    "    'https://s3.amazonaws.com/mybucket/data1.json',\n",
    "    'https://s3.amazonaws.com/mybucket/data2.json',\n",
    "    # ... more URLs ...\n",
    "]\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls, max_workers=10)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import io\n",
    "from utils.s3utils import S3Utils\n",
    "\n",
    "s3utils = S3Utils()\n",
    "\n",
    "def load_jsons_to_df_with_cudf(urls):\n",
    "    # List to store each DataFrame\n",
    "    gdf_list = []\n",
    "    \n",
    "    # Iterate over all S3 URLs\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "        # Assuming `download_file_get_content` downloads the file content as a bytes object\n",
    "        content = s3utils.download_file_get_content(url)\n",
    "        if content:\n",
    "            # Read JSON file into cuDF DataFrame\n",
    "            # since `cudf.read_json` expects a JSON lines format, we directly use `content`\n",
    "            gdf_list.append(cudf.read_json(io.BytesIO(content), lines=True))\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    # If there's anything in the list, use cudf.concat, otherwise just create an empty DataFrame\n",
    "    full_gdf = cudf.concat(gdf_list, ignore_index=True) if gdf_list else cudf.DataFrame()\n",
    "    return full_gdf\n",
    "\n",
    "# Example usage\n",
    "# `list_urls` should be a list of presigned S3 URLs pointing to the JSON files\n",
    "\n",
    "# Load the JSON files into a single cuDF DataFrame\n",
    "gdf = load_jsons_to_df_with_cudf(list_urls)\n",
    "print(gdf.head())  # Print the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"2.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "gdf = cudf.read_csv(\"mock_data.csv\")\n",
    "print(gdf.head()) # Print the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_gdf(gdf, transformations):\n",
    "    for transform in transformations:\n",
    "        from_column = transform.get('from_column_name')\n",
    "        to_column = transform.get('to_column_name')\n",
    "        type_update = transform.get('type_update')  # Get the type if it exists\n",
    "        transformation_function = transform.get('transformation_function')\n",
    "\n",
    "        if transformation_function and from_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[from_column].applymap(transformation_function)\n",
    "\n",
    "        # Rename the column if necessary\n",
    "        if from_column in gdf.columns and from_column != to_column:\n",
    "            gdf = gdf.rename(columns={from_column: to_column})\n",
    "            \n",
    "        # Update column type if type_update is provided\n",
    "        if type_update and to_column in gdf.columns:\n",
    "            gdf[to_column] = gdf[to_column].astype(type_update)\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    {\n",
    "        'from_column_name': 'create_time',\n",
    "        'to_column_name': 'created_time'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'email',\n",
    "        'to_column_name': 'email_id'\n",
    "    },\n",
    "    {\n",
    "        'from_column_name': 'phone',\n",
    "        'to_column_name': 'phone',\n",
    "        'type_update':'float64'\n",
    "    }\n",
    "    \n",
    "    # Add as many dictionaries as needed for transformations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcdf = transform_gdf(gdf,transformations)\n",
    "print(tcdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txt_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
